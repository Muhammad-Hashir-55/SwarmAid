from langchain.agents import initialize_agent, Tool
from langchain_openai import ChatOpenAI  # ‚úÖ AIML API wrapper
from langchain.schema import OutputParserException
from .keys import ss   # ‚úÖ AIML API key

# ‚úÖ Load AIML API key
AIML_API_KEY = ss

# ‚úÖ Setup LLM (GPT-5 via AIML API)
llm = ChatOpenAI(
    model="gpt-5-chat-latest",
    api_key=AIML_API_KEY,
    base_url="https://api.aimlapi.com/v1",
    temperature=0.2
)

# --- Define tools ---
def analyze_tweets(query: str) -> str:
    # üîπ Placeholder for now, later integrate real Twitter API / cached JSON
    return f"(Pretend triage from tweets: {query})"

tools = [
    Tool(
        name="Tweet Analyzer",
        func=analyze_tweets,
        description="Analyze Twitter hashtags for medical triage info"
    )
]

# ‚úÖ Build Medic Coordinator Agent
medic_coordinator = initialize_agent(
    tools,
    llm,
    agent="zero-shot-react-description",
    verbose=True,
    handle_parsing_errors=True
)

# # --- Optional interactive loop ---
# print("üöë Chat with Medic Coordinator Agent (type 'exit' to quit)\n")
# print("Role: Emergency Medic Coordinator")
# print("Goal: Use social media (Twitter/X signals) to assist in medical triage\n")
#
# while True:
#     user_input = input("You: ")
#     if user_input.lower() in ["exit", "quit"]:
#         print("üëã Exiting...")
#         break
#
#     try:
#         response = medic_coordinator.run(user_input)
#         print(f"\nMedic Coordinator: {response}\n")
#     except OutputParserException as e:
#         print(f"\n‚ö†Ô∏è Parsing issue, showing raw LLM response:\n{str(e)}\n")
#         try:
#             raw_response = llm.invoke(user_input)
#             print(f"Medic Coordinator (raw): {raw_response.content}\n")
#         except Exception as inner_e:
#             print(f"‚ùå Fallback also failed: {inner_e}\n")
#     except Exception as e:
#         print(f"‚ö†Ô∏è Unexpected error: {e}\n")
